# -*- coding: utf-8 -*-
"""Nuevos modelos con GridSearch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XINXxF73AW6QVvZr6Wbgi8R5dl-KMQar

# Librerias
"""

import numpy as np
import matplotlib.pyplot as plt
import math
import seaborn as sn
import pandas as pd
from math import e
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn import linear_model
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import  accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import  roc_curve
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestClassifier
from sklearn import preprocessing
from sklearn.model_selection import StratifiedShuffleSplit
from mlxtend.plotting import plot_confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score
from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, roc_curve
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.pipeline import Pipeline
from sklearn.model_selection import RepeatedStratifiedKFold
import joblib

"""# Pruebas"""

import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import label_binarize
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn import metrics
from sklearn.metrics import accuracy_score,roc_auc_score
from xgboost.sklearn import XGBClassifier

"""## G1"""

# Se carga la data seleccionada para el estudio
csv_path = '/content/TrainG1.csv'
TrainG1 = pd.read_csv(csv_path, sep=',')

TrainG1.rename({"Unnamed: 0":"a"}, axis="columns", inplace=True)
TrainG1.drop(["a"], axis=1, inplace=True)

TrainG1.Respiracion = TrainG1.Respiracion.replace(["No valido"], -1)
TrainG1.astype({'Respiracion':'float64'}).dtypes

# Se escogen la variables en X que se van a verificar para el analisis, y en y para el atributo que queremos identificar (De la data de entrenamiento)

y = (TrainG1["SepsisLabel"])
X = (TrainG1.drop(['SepsisLabel','Paciente','Respiracion'], axis=1))

#División del dataset con split entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30, random_state=4, stratify=y)

print(u'Dimensiones en train \n-X:{}\-Y{}'.format(X_train.shape, y_train.shape))
print(u'Dimensiones en test \n-X:{}\-Y{}'.format(X_test.shape, y_test.shape))

"""### GBDT"""

Gbdt1=GradientBoostingClassifier(n_estimators = 19, random_state = 2016,min_samples_leaf = 8,) #CBDT
#Gbdt1.fit(X_train,y_train)

# define evaluation
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)

# define search space
space = dict()
space['max_features'] = ['auto', 'sqrt', 'log2']
space['max_depth'] = [None, 1, 3, 5, 10, 20]
space['subsample'] = [0.5, 1]
space['learning_rate'] = [0.001, 0.01, 0.1]

# define search
search = GridSearchCV(Gbdt1, space, scoring='accuracy', n_jobs=-1, cv=cv)

# execute search
result = search.fit(X_train, y_train)

# summarize result
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)

y_test_hot1 = label_binarize (y_test, classes = (0, 1)) # Convierta los datos de la etiqueta del conjunto de prueba en una matriz mediante codificación binaria
Gbdt_y_score1 = search.decision_function (X_test) # Obtenga el valor de pérdida previsto de Gbdt
Gbdt_fpr1, Gbdt_tpr1, Gbdt_threasholds1 = metrics.roc_curve (y_test_hot1.ravel (), Gbdt_y_score1.ravel ()) # Calcular el valor ROC, Gbdt_threasholds es el umbral

# Se realiza la predicción
y_predGbdt1 = search.predict(X_test)
y_predGbdt1

Gbdt_score1 = search.score (X_train, y_train) # Tasa de precisión
print('Gbdt_score:',Gbdt_score1)

Gbdt_f11 = f1_score(y_test, y_predGbdt1)
print(Gbdt_f11)

Gbdt_auc1 = metrics.auc (Gbdt_fpr1, Gbdt_tpr1) #Gbdt_auc value
print('Gbdt_auc:',Gbdt_auc1)

#Matriz de Confusión
cm1= confusion_matrix(y_test, y_predGbdt1)
cm1

plot_confusion_matrix(conf_mat=cm1, figsize=(5,5), show_normed= False, cmap='Set2')
plt.tight_layout()

#Curva AUC

plt.plot(Gbdt_fpr1,Gbdt_tpr1)
plt.plot([0,1], [0,1], "r--")
plt.title("ROC GBDT")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.show()

joblib.dump(search, 'ModeloG1_GBDT.pkl')

"""### XGBC"""

Xgbc1=XGBClassifier(random_state=2018)  #Xgbc
#Xgbc1.fit(X_train,y_train)

cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)

# define search space
space = dict()
space['max_depth'] = [None, 1, 3, 5, 10, 20]
space['subsample'] = [0.5, 1]
space['learning_rate'] = [0.001, 0.01, 0.1]
space['booster'] = ['gbtree']

# define search
search = GridSearchCV(Xgbc1, space, scoring='accuracy', n_jobs=-1, cv=cv)

# execute search
result = search.fit(X_train, y_train)
y_xgbc_pred1=search.predict(X_test)

# summarize result
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)

Xgbc_score1 = precision_score (y_test, y_xgbc_pred1) # Tasa de precisión
print('Xgbc_score:',Xgbc_score1)

Xgbc_f11 = f1_score(y_test, y_xgbc_pred1)
print(Xgbc_f11)

Xgbc_auc1 = roc_auc_score (y_test, y_xgbc_pred1) #Xgbc_auc value
print('Xgbc_auc:',Xgbc_auc1)

#Matriz de Confusión
cmxgbc1= confusion_matrix(y_test, y_xgbc_pred1)
cmxgbc1

plot_confusion_matrix(conf_mat=cmxgbc1, figsize=(5,5), show_normed= False, cmap='Set2')
plt.tight_layout()

y_test_probXgbc1 = search.predict_proba(X_test)
fprXgbc1, tprXgbc1, thrsXgbc1 = roc_curve(y_test, y_test_probXgbc1[:,1])

#Curva AUC

plt.plot(fprXgbc1,tprXgbc1)
plt.plot([0,1], [0,1], "r--")
plt.title("ROC XGBC")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.show()

joblib.dump(search, 'ModeloG1_XGBC.pkl')

"""### LGBMC"""

gbm1=lgb.LGBMClassifier(random_state=2018)  #lgb
#gbm1.fit(X_train,y_train)

# define evaluation
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)

# define search space
space = dict()
space['n_estimators'] = [100, 500, 1000]
space['max_depth'] = [-1, 1, 3, 5, 10, 20]
space['subsample'] = [0.5, 1]
space['learning_rate'] = [0.001, 0.01, 0.1]
space['boosting_type'] = ['gbdt']

# define search
search = GridSearchCV(gbm1, space, scoring='accuracy', n_jobs=-1, cv=cv)

# execute search
result = search.fit(X_train, y_train)
y_gbm_pred1=search.predict(X_test)

# summarize result
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)

gbm_score1 = precision_score (y_test, y_gbm_pred1) # Tasa de precisión
print('gbm_score:',gbm_score1)

gbm_auc1 = roc_auc_score (y_test, y_gbm_pred1) #gbm_auc value
print('gbm_auc:',gbm_auc1)

gbm_f11 = f1_score(y_test, y_gbm_pred1)
print(gbm_f11)

#Matriz de Confusión
cmgbm1= confusion_matrix(y_test, y_gbm_pred1)
cmgbm1

plot_confusion_matrix(conf_mat=cmgbm1,figsize=(5,5), show_normed= False, cmap='Set2')
plt.tight_layout()

y_test_probgbm1 = search.predict_proba(X_test)
fprgbm1, tprgbm1, thrsgbm1 = roc_curve(y_test, y_test_probgbm1[:,1])

#Curva AUC

plt.plot(fprgbm1,tprgbm1)
plt.plot([0,1], [0,1], "r--")
plt.title("ROC LGBMC")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.show()

joblib.dump(search, 'ModeloG1_LGBMC.pkl')

"""### Resultado en una grafica

"""

#Curva AUC

fig = plt.figure(figsize=(10,10))
fig.tight_layout()

plt.plot(fprgbm1,tprgbm1, label='GBM', color= 'green')
plt.plot(fprXgbc1,tprXgbc1,label='XGBC', color= 'orange')
plt.plot(Gbdt_fpr1,Gbdt_tpr1, label='GBDT', color= 'blue')
plt.plot([0,1], [0,1], "r--")

plt.title("ROC Resultados")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.legend()
plt.show()



"""## G2"""

# Se carga la data seleccionada para el estudio
csv_path = '/content/TrainG2.csv'
TrainG2 = pd.read_csv(csv_path, sep=',')

TrainG2.rename({"Unnamed: 0":"a"}, axis="columns", inplace=True)
TrainG2.drop(["a"], axis=1, inplace=True)

TrainG2.Respiracion = TrainG2.Respiracion.replace(["No valido"], -1)
TrainG2.astype({'Respiracion':'float64'}).dtypes

# Se escogen la variables en X que se van a verificar para el analisis, y en y para el atributo que queremos identificar (De la data de entrenamiento)

y = (TrainG2["SepsisLabel"])
X = (TrainG2.drop(['SepsisLabel','Paciente','Respiracion'], axis=1))

#División del dataset con split entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30, random_state=4, stratify=y)

print(u'Dimensiones en train \n-X:{}\-Y{}'.format(X_train.shape, y_train.shape))
print(u'Dimensiones en test \n-X:{}\-Y{}'.format(X_test.shape, y_test.shape))

"""### GBDT"""

Gbdt2=GradientBoostingClassifier(n_estimators = 19, random_state = 2016,min_samples_leaf = 8,) #CBDT
#Gbdt1.fit(X_train,y_train)

# define evaluation
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)

# define search space
space = dict()
space['max_features'] = ['auto', 'sqrt', 'log2']
space['max_depth'] = [None, 1, 3, 5, 10, 20]
space['subsample'] = [0.5, 1]
space['learning_rate'] = [0.001, 0.01, 0.1]

# define search
search = GridSearchCV(Gbdt2, space, scoring='accuracy', n_jobs=-1, cv=cv)

# execute search
result = search.fit(X_train, y_train)

# summarize result
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)

y_test_hot2 = label_binarize (y_test, classes = (0, 1)) # Convierta los datos de la etiqueta del conjunto de prueba en una matriz mediante codificación binaria
Gbdt_y_score2 = search.decision_function (X_test) # Obtenga el valor de pérdida previsto de Gbdt
Gbdt_fpr2, Gbdt_tpr2, Gbdt_threasholds2 = metrics.roc_curve (y_test_hot2.ravel (), Gbdt_y_score2.ravel ()) # Calcular el valor ROC, Gbdt_threasholds es el umbral

# Se realiza la predicción
y_predGbdt2 = search.predict(X_test)
y_predGbdt2

Gbdt_score2 = search.score (X_train, y_train) # Tasa de precisión
print('Gbdt_score:',Gbdt_score2)

Gbdt_f12 = f1_score(y_test, y_predGbdt2)
print(Gbdt_f12)

Gbdt_auc2 = metrics.auc (Gbdt_fpr2, Gbdt_tpr2) #Gbdt_auc value
print('Gbdt_auc:',Gbdt_auc2)

#Matriz de Confusión
cm2= confusion_matrix(y_test, y_predGbdt2)
cm2

plot_confusion_matrix(conf_mat=cm2,figsize=(5,5), show_normed= False, cmap='Set2')
plt.tight_layout()

#Curva AUC

plt.plot(Gbdt_fpr2,Gbdt_tpr2)
plt.plot([0,1], [0,1], "r--")
plt.title("ROC GBDT")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.show()

joblib.dump(search, 'ModeloG2_GBDT.pkl')

"""### XGBC"""

Xgbc2=XGBClassifier(random_state=2018)  #Xgbc
#Xgbc1.fit(X_train,y_train)

cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)

# define search space
space = dict()
space['max_depth'] = [None, 1, 3, 5, 10, 20]
space['subsample'] = [0.5, 1]
space['learning_rate'] = [0.001, 0.01, 0.1]
space['booster'] = ['gbtree']

# define search
search = GridSearchCV(Xgbc2, space, scoring='accuracy', n_jobs=-1, cv=cv)

# execute search
result = search.fit(X_train, y_train)
y_xgbc_pred2=search.predict(X_test)

# summarize result
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)

y_xgbc_pred2=search.predict(X_test)

Xgbc_score2 = precision_score (y_test, y_xgbc_pred2) # Tasa de precisión
print('Xgbc_score:',Xgbc_score2)

Xgbc_f12 = f1_score(y_test, y_xgbc_pred2)
print(Xgbc_f12)

Xgbc_auc2 = roc_auc_score (y_test, y_xgbc_pred2) #Xgbc_auc value
print('Xgbc_auc:',Xgbc_auc2)

#Matriz de Confusión
cmxgbc2= confusion_matrix(y_test, y_xgbc_pred2)
cmxgbc2

plot_confusion_matrix(conf_mat=cmxgbc2, figsize=(5,5), show_normed= False, cmap='Set2')
plt.tight_layout()

y_test_probXgbc2 = search.predict_proba(X_test)
fprXgbc2, tprXgbc2, thrsXgbc2 = roc_curve(y_test, y_test_probXgbc2[:,1])

#Curva AUC

plt.plot(fprXgbc2,tprXgbc2)
plt.plot([0,1], [0,1], "r--")
plt.title("ROC XGBC")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.show()

joblib.dump(search, 'ModeloG2_XGBC.pkl')

"""### LGBMC"""

gbm2=lgb.LGBMClassifier(random_state=2018)  #lgb
#gbm1.fit(X_train,y_train)

# define evaluation
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)

# define search space
space = dict()
space['n_estimators'] = [100, 500, 1000]
space['max_depth'] = [-1, 1, 3, 5, 10, 20]
space['subsample'] = [0.5, 1]
space['learning_rate'] = [0.001, 0.01, 0.1]
space['boosting_type'] = ['gbdt']

# define search
search = GridSearchCV(gbm2, space, scoring='accuracy', n_jobs=-1, cv=cv)

# execute search
result = search.fit(X_train, y_train)
y_gbm_pred2=search.predict(X_test)

# summarize result
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)

gbm_score2 = precision_score (y_test, y_gbm_pred2) # Tasa de precisión
print('gbm_score:',gbm_score2)

gbm_auc2 = roc_auc_score (y_test, y_gbm_pred2) #gbm_auc value
print('gbm_auc:',gbm_auc2)

gbm_f12 = f1_score(y_test, y_gbm_pred2)
print(gbm_f12)

#Matriz de Confusión
cmgbm2= confusion_matrix(y_test, y_gbm_pred2)
cmgbm2

plot_confusion_matrix(conf_mat=cmgbm2,figsize=(5,5), show_normed= False, cmap='Set2')
plt.tight_layout()

y_test_probgbm2 = search.predict_proba(X_test)
fprgbm2, tprgbm2, thrsgbm2 = roc_curve(y_test, y_test_probgbm2[:,1])

#Curva AUC

plt.plot(fprgbm2,tprgbm2)
plt.plot([0,1], [0,1], "r--")
plt.title("ROC LGBMC")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.show()

joblib.dump(search, 'ModeloG2_LGBMC.pkl')

"""### Resultado en una grafica

"""

#Curva AUC

fig = plt.figure(figsize=(10,10))
fig.tight_layout()

plt.plot(fprgbm2,tprgbm2, label='GBM', color= 'green')
plt.plot(fprXgbc2,tprXgbc2,label='XGBC', color= 'orange')
plt.plot(Gbdt_fpr2,Gbdt_tpr2, label='GBDT', color= 'blue')
plt.plot([0,1], [0,1], "r--")

plt.title("ROC Resultados")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.legend()
plt.show()



"""## G3"""

# Se carga la data seleccionada para el estudio
csv_path = '/content/TrainG3.csv'
TrainG3 = pd.read_csv(csv_path, sep=',')

TrainG3.rename({"Unnamed: 0":"a"}, axis="columns", inplace=True)
TrainG3.drop(["a"], axis=1, inplace=True)

TrainG3.Respiracion = TrainG3.Respiracion.replace(["No valido"], -1)
TrainG3.astype({'Respiracion':'float64'}).dtypes

# Se escogen la variables en X que se van a verificar para el analisis, y en y para el atributo que queremos identificar (De la data de entrenamiento)

y = (TrainG3["SepsisLabel"])
X = (TrainG3.drop(['SepsisLabel','Paciente','Respiracion'], axis=1))

#División del dataset con split entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30, random_state=4, stratify=y)

print(u'Dimensiones en train \n-X:{}\-Y{}'.format(X_train.shape, y_train.shape))
print(u'Dimensiones en test \n-X:{}\-Y{}'.format(X_test.shape, y_test.shape))

"""### GBDT"""

Gbdt3=GradientBoostingClassifier(n_estimators = 19, random_state = 2016,min_samples_leaf = 8,) #CBDT
#Gbdt1.fit(X_train,y_train)

# define evaluation
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)

# define search space
space = dict()
space['max_features'] = ['auto', 'sqrt', 'log2']
space['max_depth'] = [None, 1, 3, 5, 10, 20]
space['subsample'] = [0.5, 1]
space['learning_rate'] = [0.001, 0.01, 0.1]

# define search
search = GridSearchCV(Gbdt3, space, scoring='accuracy', n_jobs=-1, cv=cv)

# execute search
result = search.fit(X_train, y_train)

# summarize result
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)

y_test_hot3 = label_binarize (y_test, classes = (0, 1)) # Convierta los datos de la etiqueta del conjunto de prueba en una matriz mediante codificación binaria
Gbdt_y_score3 = search.decision_function (X_test) # Obtenga el valor de pérdida previsto de Gbdt
Gbdt_fpr3, Gbdt_tpr3, Gbdt_threasholds3 = metrics.roc_curve (y_test_hot3.ravel (), Gbdt_y_score3.ravel ()) # Calcular el valor ROC, Gbdt_threasholds es el umbral

# Se realiza la predicción
y_predGbdt3 = search.predict(X_test)
y_predGbdt3

Gbdt_score3 = search.score (X_train, y_train) # Tasa de precisión
print('Gbdt_score:',Gbdt_score3)

Gbdt_f13 = f1_score(y_test, y_predGbdt3)
print(Gbdt_f13)

Gbdt_auc3 = metrics.auc (Gbdt_fpr3, Gbdt_tpr3) #Gbdt_auc value
print('Gbdt_auc:',Gbdt_auc3)

#Matriz de Confusión
cm3= confusion_matrix(y_test, y_predGbdt3)
cm3

plot_confusion_matrix(conf_mat=cm3,figsize=(5,5), show_normed= False, cmap='Set2')
plt.tight_layout()

#Curva AUC

plt.plot(Gbdt_fpr3,Gbdt_tpr3)
plt.plot([0,1], [0,1], "r--")
plt.title("ROC GBDT")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.show()

joblib.dump(search, 'ModeloG3_GBDT.pkl')

"""### XGBC"""

Xgbc3=XGBClassifier(random_state=2018)  #Xgbc
#Xgbc1.fit(X_train,y_train)

cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)

# define search space
space = dict()
space['max_depth'] = [None, 1, 3, 5, 10, 20]
space['subsample'] = [0.5, 1]
space['learning_rate'] = [0.001, 0.01, 0.1]
space['booster'] = ['gbtree']

# define search
search = GridSearchCV(Xgbc3, space, scoring='accuracy', n_jobs=-1, cv=cv)

# execute search
result = search.fit(X_train, y_train)
y_xgbc_pred3=search.predict(X_test)

# summarize result
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)

Xgbc_score3 = precision_score (y_test, y_xgbc_pred3) # Tasa de precisión
print('Xgbc_score:',Xgbc_score3)

Xgbc_f13 = f1_score(y_test, y_xgbc_pred3)
print(Xgbc_f13)

Xgbc_auc3 = roc_auc_score (y_test, y_xgbc_pred3) #Xgbc_auc value
print('Xgbc_auc:',Xgbc_auc3)

#Matriz de Confusión
cmxgbc3= confusion_matrix(y_test, y_xgbc_pred3)
cmxgbc3

plot_confusion_matrix(conf_mat=cmxgbc3, figsize=(5,5), show_normed= False, cmap='Set2')
plt.tight_layout()

y_test_probXgbc3 = search.predict_proba(X_test)
fprXgbc3, tprXgbc3, thrsXgbc3 = roc_curve(y_test, y_test_probXgbc3[:,1])

#Curva AUC

plt.plot(fprXgbc3,tprXgbc3)
plt.plot([0,1], [0,1], "r--")
plt.title("ROC XGBC")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.show()

joblib.dump(search, 'ModeloG3_XGBC.pkl')

"""### LGBMC"""

gbm3=lgb.LGBMClassifier(random_state=2018)  #lgb
#gbm1.fit(X_train,y_train)

# define evaluation
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)

# define search space
space = dict()
space['n_estimators'] = [100, 500, 1000]
space['max_depth'] = [-1, 1, 3, 5, 10, 20]
space['subsample'] = [0.5, 1]
space['learning_rate'] = [0.001, 0.01, 0.1]
space['boosting_type'] = ['gbdt']

# define search
search = GridSearchCV(gbm3, space, scoring='accuracy', n_jobs=-1, cv=cv)

# execute search
result = search.fit(X_train, y_train)
y_gbm_pred3=search.predict(X_test)

# summarize result
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)

gbm_score3 = precision_score (y_test, y_gbm_pred3) # Tasa de precisión
print('gbm_score:',gbm_score3)

gbm_auc3 = roc_auc_score (y_test, y_gbm_pred3) #gbm_auc value
print('gbm_auc:',gbm_auc3)

gbm_f13 = f1_score(y_test, y_gbm_pred3)
print(gbm_f13)

#Matriz de Confusión
cmgbm3= confusion_matrix(y_test, y_gbm_pred3)
cmgbm3

plot_confusion_matrix(conf_mat=cmgbm3, figsize=(5,5), show_normed= False, cmap='Set2')
plt.tight_layout()

y_test_probgbm3 = search.predict_proba(X_test)
fprgbm3, tprgbm3, thrsgbm3 = roc_curve(y_test, y_test_probgbm3[:,1])

#Curva AUC

plt.plot(fprgbm3,tprgbm3)
plt.plot([0,1], [0,1], "r--")
plt.title("ROC LGBMC")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.show()

joblib.dump(search, 'ModeloG3_LGBMC.pkl')

"""### Resultado en una grafica

"""

#Curva AUC

fig = plt.figure(figsize=(10,10))
fig.tight_layout()

plt.plot(fprgbm3,tprgbm3, label='GBM', color= 'green')
plt.plot(fprXgbc3,tprXgbc3,label='XGBC', color= 'orange')
plt.plot(Gbdt_fpr3,Gbdt_tpr3, label='GBDT', color= 'blue')
plt.plot([0,1], [0,1], "r--")

plt.title("ROC Resultados")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.legend()
plt.show()



"""## G4"""

# Se carga la data seleccionada para el estudio
csv_path = '/content/TrainG4.csv'
TrainG4 = pd.read_csv(csv_path, sep=',')

TrainG4.rename({"Unnamed: 0":"a"}, axis="columns", inplace=True)
TrainG4.drop(["a"], axis=1, inplace=True)

TrainG4.Respiracion = TrainG4.Respiracion.replace(["No valido"], -1)
TrainG4.astype({'Respiracion':'float64'}).dtypes

# Se escogen la variables en X que se van a verificar para el analisis, y en y para el atributo que queremos identificar (De la data de entrenamiento)

y = (TrainG4["SepsisLabel"])
X = (TrainG4.drop(['SepsisLabel','Paciente','Respiracion'], axis=1))

#División del dataset con split entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30, random_state=4, stratify=y)

print(u'Dimensiones en train \n-X:{}\-Y{}'.format(X_train.shape, y_train.shape))
print(u'Dimensiones en test \n-X:{}\-Y{}'.format(X_test.shape, y_test.shape))

"""### GBDT"""

Gbdt4=GradientBoostingClassifier(n_estimators = 19, random_state = 2016,min_samples_leaf = 8,) #CBDT
#Gbdt1.fit(X_train,y_train)

# define evaluation
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)

# define search space
space = dict()
space['max_features'] = ['auto', 'sqrt', 'log2']
space['max_depth'] = [None, 1, 3, 5, 10, 20]
space['subsample'] = [0.5, 1]
space['learning_rate'] = [0.001, 0.01, 0.1]

# define search
search = GridSearchCV(Gbdt4, space, scoring='accuracy', n_jobs=-1, cv=cv)

# execute search
result = search.fit(X_train, y_train)

# summarize result
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)

y_test_hot4 = label_binarize (y_test, classes = (0, 1)) # Convierta los datos de la etiqueta del conjunto de prueba en una matriz mediante codificación binaria
Gbdt_y_score4 = search.decision_function (X_test) # Obtenga el valor de pérdida previsto de Gbdt
Gbdt_fpr4, Gbdt_tpr4, Gbdt_threasholds4 = metrics.roc_curve (y_test_hot4.ravel (), Gbdt_y_score4.ravel ()) # Calcular el valor ROC, Gbdt_threasholds es el umbral

# Se realiza la predicción
y_predGbdt4 = search.predict(X_test)
y_predGbdt4

Gbdt_score4 = search.score (X_train, y_train) # Tasa de precisión
print('Gbdt_score:',Gbdt_score4)

Gbdt_f14 = f1_score(y_test, y_predGbdt4)
print(Gbdt_f14)

Gbdt_auc4 = metrics.auc (Gbdt_fpr4, Gbdt_tpr4) #Gbdt_auc value
print('Gbdt_auc:',Gbdt_auc4)

#Matriz de Confusión
cm4= confusion_matrix(y_test, y_predGbdt4)
cm4

plot_confusion_matrix(conf_mat=cm4, figsize=(5,5), show_normed= False, cmap='Set2')
plt.tight_layout()

#Curva AUC

plt.plot(Gbdt_fpr4,Gbdt_tpr4)
plt.plot([0,1], [0,1], "r--")
plt.title("ROC GBDT")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.show()

joblib.dump(search, 'ModeloG4_GBDT.pkl')

"""### XGBC"""

Xgbc4=XGBClassifier(random_state=2018)  #Xgbc
#Xgbc1.fit(X_train,y_train)

cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)

# define search space
space = dict()
space['max_depth'] = [None, 1, 3, 5, 10, 20]
space['subsample'] = [0.5, 1]
space['learning_rate'] = [0.001, 0.01, 0.1]
space['booster'] = ['gbtree']

# define search
search = GridSearchCV(Xgbc4, space, scoring='accuracy', n_jobs=-1, cv=cv)

# execute search
result = search.fit(X_train, y_train)
y_xgbc_pred4=search.predict(X_test)

# summarize result
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)

Xgbc_score4 = precision_score (y_test, y_xgbc_pred4) # Tasa de precisión
print('Xgbc_score:',Xgbc_score4)

Xgbc_f14 = f1_score(y_test, y_xgbc_pred4)
print(Xgbc_f14)

Xgbc_auc4 = roc_auc_score (y_test, y_xgbc_pred4) #Xgbc_auc value
print('Xgbc_auc:',Xgbc_auc4)

#Matriz de Confusión
cmxgbc4= confusion_matrix(y_test, y_xgbc_pred4)
cmxgbc4

plot_confusion_matrix(conf_mat=cmxgbc4, figsize=(5,5), show_normed= False, cmap='Set2')
plt.tight_layout()

y_test_probXgbc4 = search.predict_proba(X_test)
fprXgbc4, tprXgbc4, thrsXgbc4 = roc_curve(y_test, y_test_probXgbc4[:,1])

#Curva AUC

plt.plot(fprXgbc4,tprXgbc4)
plt.plot([0,1], [0,1], "r--")
plt.title("ROC XGBC")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.show()

joblib.dump(search, 'ModeloG4_XGBC.pkl')

"""### LGBMC"""

gbm4=lgb.LGBMClassifier(random_state=2018)  #lgb
#gbm1.fit(X_train,y_train)

# define evaluation
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)

# define search space
space = dict()
space['n_estimators'] = [100, 500, 1000]
space['max_depth'] = [-1, 1, 3, 5, 10, 20]
space['subsample'] = [0.5, 1]
space['learning_rate'] = [0.001, 0.01, 0.1]
space['boosting_type'] = ['gbdt']

# define search
search = GridSearchCV(gbm4, space, scoring='accuracy', n_jobs=-1, cv=cv)

# execute search
result = search.fit(X_train, y_train)
y_gbm_pred4=search.predict(X_test)

# summarize result
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)

gbm_score4 = precision_score (y_test, y_gbm_pred4) # Tasa de precisión
print('gbm_score:',gbm_score4)

gbm_auc4 = roc_auc_score (y_test, y_gbm_pred4) #gbm_auc value
print('gbm_auc:',gbm_auc4)

gbm_f14 = f1_score(y_test, y_gbm_pred4)
print(gbm_f14)

#Matriz de Confusión
cmgbm4= confusion_matrix(y_test, y_gbm_pred4)
cmgbm4

plot_confusion_matrix(conf_mat=cmgbm4,figsize=(5,5), show_normed= False, cmap='Set2')
plt.tight_layout()

y_test_probgbm4 = search.predict_proba(X_test)
fprgbm4, tprgbm4, thrsgbm4 = roc_curve(y_test, y_test_probgbm4[:,1])

#Curva AUC

plt.plot(fprgbm4,tprgbm4)
plt.plot([0,1], [0,1], "r--")
plt.title("ROC LGBMC")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.show()

joblib.dump(search, 'ModeloG4_LGBMC.pkl')

"""### Resultado en una grafica

"""

#Curva AUC

fig = plt.figure(figsize=(10,10))
fig.tight_layout()

plt.plot(fprgbm4,tprgbm4, label='GBM', color= 'green')
plt.plot(fprXgbc4,tprXgbc4,label='XGBC', color= 'orange')
plt.plot(Gbdt_fpr4,Gbdt_tpr4, label='GBDT', color= 'blue')
plt.plot([0,1], [0,1], "r--")

plt.title("ROC Resultados")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.legend()
plt.show()



"""## GRUPO COMPLETO"""

# Se carga la data seleccionada para el estudio
csv_path = '/content/TrainCompleto.csv'
TrainCompleto = pd.read_csv(csv_path, sep=',')

TrainCompleto.rename({"Unnamed: 0":"a"}, axis="columns", inplace=True)
TrainCompleto.drop(["a"], axis=1, inplace=True)

TrainCompleto.Respiracion = TrainCompleto.Respiracion.replace(["No valido"], -1)
TrainCompleto.astype({'Respiracion':'float64'}).dtypes

# Se escogen la variables en X que se van a verificar para el analisis, y en y para el atributo que queremos identificar (De la data de entrenamiento)

y = (TrainCompleto["SepsisLabel"])
X = (TrainCompleto.drop(['SepsisLabel','Paciente','Respiracion'], axis=1))

#División del dataset con split entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30, random_state=4, stratify=y)

print(u'Dimensiones en train \n-X:{}\-Y{}'.format(X_train.shape, y_train.shape))
print(u'Dimensiones en test \n-X:{}\-Y{}'.format(X_test.shape, y_test.shape))

"""### GBDT"""

Gbdt1=GradientBoostingClassifier(n_estimators = 19, random_state = 2016,min_samples_leaf = 8,) #CBDT
#Gbdt1.fit(X_train,y_train)

# define evaluation
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)

# define search space
space = dict()
space['max_features'] = ['auto', 'sqrt', 'log2']
space['max_depth'] = [None, 1, 3, 5, 10, 20]
space['subsample'] = [0.5, 1]
space['learning_rate'] = [0.001, 0.01, 0.1]

# define search
search = GridSearchCV(Gbdt1, space, scoring='accuracy', n_jobs=-1, cv=cv)

# execute search
result = search.fit(X_train, y_train)

# summarize result
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)

y_test_hot1 = label_binarize (y_test, classes = (0, 1)) # Convierta los datos de la etiqueta del conjunto de prueba en una matriz mediante codificación binaria
Gbdt_y_score1 = search.decision_function (X_test) # Obtenga el valor de pérdida previsto de Gbdt
Gbdt_fpr1, Gbdt_tpr1, Gbdt_threasholds1 = metrics.roc_curve (y_test_hot1.ravel (), Gbdt_y_score1.ravel ()) # Calcular el valor ROC, Gbdt_threasholds es el umbral

# Se realiza la predicción
y_predGbdt1 = search.predict(X_test)
y_predGbdt1

Gbdt_score1 = search.score (X_train, y_train) # Tasa de precisión
print('Gbdt_score:',Gbdt_score1)

Gbdt_f11 = f1_score(y_test, y_predGbdt1)
print(Gbdt_f11)

Gbdt_auc1 = metrics.auc (Gbdt_fpr1, Gbdt_tpr1) #Gbdt_auc value
print('Gbdt_auc:',Gbdt_auc1)

#Matriz de Confusión
cm1= confusion_matrix(y_test, y_predGbdt1)
cm1

plot_confusion_matrix(conf_mat=cm1, figsize=(5,5), show_normed= False, cmap='Set2')
plt.tight_layout()

#Curva AUC

plt.plot(Gbdt_fpr1,Gbdt_tpr1)
plt.plot([0,1], [0,1], "r--")
plt.title("ROC GBDT")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.show()

joblib.dump(Gbdt1, 'ModeloG1_GBDT.pkl')

"""### XGBC"""

Xgbc1=XGBClassifier(random_state=2018)  #Xgbc
Xgbc1.fit(X_train,y_train)
y_xgbc_pred1=Xgbc1.predict(X_test)

Xgbc_score1 = precision_score (y_test, y_xgbc_pred1) # Tasa de precisión
print('Xgbc_score:',Xgbc_score1)

Xgbc_f11 = f1_score(y_test, y_xgbc_pred1)
print(Xgbc_f11)

Xgbc_auc1 = roc_auc_score (y_test, y_xgbc_pred1) #Xgbc_auc value
print('Xgbc_auc:',Xgbc_auc1)

#Matriz de Confusión
cmxgbc1= confusion_matrix(y_test, y_xgbc_pred1)
cmxgbc1

plot_confusion_matrix(conf_mat=cmxgbc1, figsize=(5,5), show_normed= False, cmap='Set2')
plt.tight_layout()

y_test_probXgbc1 = Xgbc1.predict_proba(X_test)
fprXgbc1, tprXgbc1, thrsXgbc1 = roc_curve(y_test, y_test_probXgbc1[:,1])

#Curva AUC

plt.plot(fprXgbc1,tprXgbc1)
plt.plot([0,1], [0,1], "r--")
plt.title("ROC XGBC")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.show()

joblib.dump(Xgbc1, 'ModeloG1_XGBC.pkl')

"""### LGBMC"""

gbm1=lgb.LGBMClassifier(random_state=2018)  #lgb
#gbm1.fit(X_train,y_train)

# define evaluation
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)

# define search space
space = dict()
space['n_estimators'] = [100, 500, 1000]
space['max_depth'] = [-1, 1, 3, 5, 10, 20]
space['subsample'] = [0.5, 1]
space['learning_rate'] = [0.001, 0.01, 0.1]
space['boosting_type'] = ['gbdt']

# define search
search = GridSearchCV(gbm1, space, scoring='accuracy', n_jobs=-1, cv=cv)

# execute search
result = search.fit(X_train, y_train)

# summarize result
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)

y_gbm_pred1=search.predict(X_test)

gbm_score1 = precision_score (y_test, y_gbm_pred1) # Tasa de precisión
print('gbm_score:',gbm_score1)

gbm_auc1 = roc_auc_score (y_test, y_gbm_pred1) #gbm_auc value
print('gbm_auc:',gbm_auc1)

gbm_f11 = f1_score(y_test, y_gbm_pred1)
print(gbm_f11)

#Matriz de Confusión
cmgbm1= confusion_matrix(y_test, y_gbm_pred1)
cmgbm1

plot_confusion_matrix(conf_mat=cmgbm1,figsize=(5,5), show_normed= False, cmap='Set2')
plt.tight_layout()

y_test_probgbm1 = Xgbc1.predict_proba(X_test)
fprgbm1, tprgbm1, thrsgbm1 = roc_curve(y_test, y_test_probgbm1[:,1])

#Curva AUC

plt.plot(fprgbm1,tprgbm1)
plt.plot([0,1], [0,1], "r--")
plt.title("ROC LGBMC")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.show()

joblib.dump(gbm1, 'ModeloG1_LGBMC.pkl')

"""### Resultado en una grafica

"""

#Curva AUC

fig = plt.figure(figsize=(10,10))
fig.tight_layout()

plt.plot(fprgbm1,tprgbm1, label='GBM', color= 'green')
plt.plot(fprXgbc1,tprXgbc1,label='XGBC', color= 'orange')
plt.plot(Gbdt_fpr1,Gbdt_tpr1, label='GBDT', color= 'blue')
plt.plot([0,1], [0,1], "r--")

plt.title("ROC Resultados")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.legend()
plt.show()



"""## GRUPO COMPLETO Train y Test"""

# Se carga la data seleccionada para el estudio

csv_path = '/content/TrainCompleto.csv'
TrainCompleto = pd.read_csv(csv_path, sep=',')

csv_path = '/content/TestCompleto.csv'
TestCompleto = pd.read_csv(csv_path, sep=',')


TrainCompleto.rename({"Unnamed: 0":"a"}, axis="columns", inplace=True)
TrainCompleto.drop(["a"], axis=1, inplace=True)

TrainCompleto.Respiracion = TrainCompleto.Respiracion.replace(["No valido"], -1)
TrainCompleto.astype({'Respiracion':'float64'}).dtypes

TestCompleto.rename({"Unnamed: 0":"a"}, axis="columns", inplace=True)
TestCompleto.drop(["a"], axis=1, inplace=True)

TestCompleto.Respiracion = TestCompleto.Respiracion.replace(["No valido"], -1)
TestCompleto.astype({'Respiracion':'float64'}).dtypes

# Se escogen la variables en X que se van a verificar para el analisis, y en y para el atributo que queremos identificar (De la data de entrenamiento)

y = (TestCompleto["SepsisLabel"])
X = (TrainCompleto.drop(['SepsisLabel','Paciente','Respiracion'], axis=1))

"""### GBDT"""

Gbdt1=GradientBoostingClassifier(random_state=2018) #CBDT
Gbdt1.fit(X,y)

y_test_hot1 = label_binarize (y_test, classes = (0, 1)) # Convierta los datos de la etiqueta del conjunto de prueba en una matriz mediante codificación binaria
Gbdt_y_score1 = Gbdt1.decision_function (X_test) # Obtenga el valor de pérdida previsto de Gbdt
Gbdt_fpr1, Gbdt_tpr1, Gbdt_threasholds1 = metrics.roc_curve (y_test_hot1.ravel (), Gbdt_y_score1.ravel ()) # Calcular el valor ROC, Gbdt_threasholds es el umbral

# Se realiza la predicción
y_predGbdt1 = Gbdt1.predict(X_test)
y_predGbdt1

Gbdt_score1 = Gbdt1.score (X_train, y_train) # Tasa de precisión
print('Gbdt_score:',Gbdt_score1)

Gbdt_f11 = f1_score(y_test, y_predGbdt1)
print(Gbdt_f11)

Gbdt_auc1 = metrics.auc (Gbdt_fpr1, Gbdt_tpr1) #Gbdt_auc value
print('Gbdt_auc:',Gbdt_auc1)

#Matriz de Confusión
cm1= confusion_matrix(y_test, y_predGbdt1)
cm1

plot_confusion_matrix(conf_mat=cm1, figsize=(5,5), show_normed= False, cmap='Set2')
plt.tight_layout()

#Curva AUC

plt.plot(Gbdt_fpr1,Gbdt_tpr1)
plt.plot([0,1], [0,1], "r--")
plt.title("ROC GBDT")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.show()

joblib.dump(Gbdt1, 'ModeloG1_GBDT.pkl')

"""### XGBC"""

Xgbc1=XGBClassifier(random_state=2018)  #Xgbc
Xgbc1.fit(X_train,y_train)
y_xgbc_pred1=Xgbc1.predict(X_test)

Xgbc_score1 = precision_score (y_test, y_xgbc_pred1) # Tasa de precisión
print('Xgbc_score:',Xgbc_score1)

Xgbc_f11 = f1_score(y_test, y_xgbc_pred1)
print(Xgbc_f11)

Xgbc_auc1 = roc_auc_score (y_test, y_xgbc_pred1) #Xgbc_auc value
print('Xgbc_auc:',Xgbc_auc1)

#Matriz de Confusión
cmxgbc1= confusion_matrix(y_test, y_xgbc_pred1)
cmxgbc1

plot_confusion_matrix(conf_mat=cmxgbc1, figsize=(5,5), show_normed= False, cmap='Set2')
plt.tight_layout()

y_test_probXgbc1 = Xgbc1.predict_proba(X_test)
fprXgbc1, tprXgbc1, thrsXgbc1 = roc_curve(y_test, y_test_probXgbc1[:,1])

#Curva AUC

plt.plot(fprXgbc1,tprXgbc1)
plt.plot([0,1], [0,1], "r--")
plt.title("ROC XGBC")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.show()

joblib.dump(Xgbc1, 'ModeloG1_XGBC.pkl')

"""### LGBMC"""

gbm1=lgb.LGBMClassifier(random_state=2018)  #lgb
gbm1.fit(X_train,y_train)
y_gbm_pred1=gbm1.predict(X_test)

gbm_score1 = precision_score (y_test, y_gbm_pred1) # Tasa de precisión
print('gbm_score:',gbm_score1)

gbm_auc1 = roc_auc_score (y_test, y_gbm_pred1) #gbm_auc value
print('gbm_auc:',gbm_auc1)

gbm_f11 = f1_score(y_test, y_gbm_pred1)
print(gbm_f11)

#Matriz de Confusión
cmgbm1= confusion_matrix(y_test, y_gbm_pred1)
cmgbm1

plot_confusion_matrix(conf_mat=cmgbm1,figsize=(5,5), show_normed= False, cmap='Set2')
plt.tight_layout()

y_test_probgbm1 = Xgbc1.predict_proba(X_test)
fprgbm1, tprgbm1, thrsgbm1 = roc_curve(y_test, y_test_probgbm1[:,1])

#Curva AUC

plt.plot(fprgbm1,tprgbm1)
plt.plot([0,1], [0,1], "r--")
plt.title("ROC LGBMC")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.show()

joblib.dump(gbm1, 'ModeloG1_LGBMC.pkl')

"""### Resultado en una grafica

"""

#Curva AUC

fig = plt.figure(figsize=(10,10))
fig.tight_layout()

plt.plot(fprgbm1,tprgbm1, label='GBM', color= 'green')
plt.plot(fprXgbc1,tprXgbc1,label='XGBC', color= 'orange')
plt.plot(Gbdt_fpr1,Gbdt_tpr1, label='GBDT', color= 'blue')
plt.plot([0,1], [0,1], "r--")

plt.title("ROC Resultados")
plt.xlabel("Falsos Positivos")
plt.ylabel ("Verdaderos Psotivos")
plt.legend()
plt.show()

